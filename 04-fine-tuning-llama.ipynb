{"cells":[{"cell_type":"markdown","metadata":{"id":"PTYyXm6Ub-bj"},"source":["## Fine-Tuning LLMs (Lllama-2)\n","\n","**By the end of the notebook you will learn**\n","- how to fine-tune llama2 model using Q-Lora\n","- Combine trained adapters with base model and save the serialized model\n","- push the weights to GCS bucket (will be used in inference later)\n","\n","**References**\n","- https://www.philschmid.de/instruction-tune-llama-2\n","- https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32\n"]},{"cell_type":"markdown","metadata":{"id":"u1WTzhpQb-bl"},"source":["### Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MLkskhOcb-bm"},"outputs":[],"source":["# install required libaries\n","!pip install -q accelerate==0.21.0 \\\n","                peft==0.4.0 \\\n","                bitsandbytes==0.40.2 \\\n","                transformers==4.31.0 \\\n","                trl==0.4.7 \\\n","                huggingface_hub \\\n","                wandb \\\n","                python-dotenv\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m7RMlmkQb-bm"},"outputs":[],"source":["import os\n","os.environ[\"WANDB_API_KEY\"]=\"PLACE_YOUR_KEY\"\n","os.environ[\"HUGGING_FACE_HUB_TOKEN\"]=\"PLACE_YOUR_KEY\"\n","# from dotenv import load_dotenv, find_dotenv\n","# _ = load_dotenv(find_dotenv())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":565,"status":"ok","timestamp":1690784806019,"user":{"displayName":"abhishek kumar","userId":"11096944369498737237"},"user_tz":-330},"id":"oG9K5X1Ub-bm","outputId":"1482333e-d680-4239-f82b-df0a6c6930c5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["# login to hugging face to get the gated model weights\n","from huggingface_hub import login\n","# make sure you have set the environment variable HUGGING_FACE_HUB_TOKEN in .env\n","login(token=os.environ[\"HUGGING_FACE_HUB_TOKEN\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rd0ZSPDCb-bn"},"outputs":[],"source":["import wandb\n","# make sure you have set the environment variable WANDB_API_KEY in .env\n","_ = wandb.login()\n","os.environ[\"WANDB_PROJECT\"] = \"fine-tuning\" # change the project name based on your need"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"292Mb0GKb-bn"},"outputs":[],"source":["# import libraries\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer\n"]},{"cell_type":"markdown","metadata":{"id":"1qT7Vvpzb-bn"},"source":["### Setup Training Arguments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KzOoXLSDb-bn"},"outputs":[],"source":["# The model that you want to train from the Hugging Face hub\n","model_name = \"meta-llama/Llama-2-7b-hf\"\n","\n","# The instruction dataset to use\n","dataset_name = \"mlabonne/guanaco-llama2-1k\"\n","\n","# Fine-tuned model name\n","new_model = \"llama-2-7b-hf-v2\"\n","\n","################################################################################\n","# QLoRA parameters\n","################################################################################\n","\n","# LoRA attention dimension\n","lora_r = 64\n","\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"./results\"\n","\n","# Number of training epochs\n","num_train_epochs = 1\n","\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = False\n","bf16 = False\n","\n","# Batch size per GPU for training\n","per_device_train_batch_size = 4\n","\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4\n","\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4\n","\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","\n","# Learning rate schedule (constant a bit better than cosine)\n","lr_scheduler_type = \"constant\"\n","\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","\n","# Save checkpoint every X updates steps\n","save_steps = 25\n","\n","# Log every X updates steps\n","logging_steps = 25\n","\n","################################################################################\n","# SFT parameters\n","################################################################################\n","\n","# Maximum sequence length to use\n","max_seq_length = None\n","\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = False\n","\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":103,"referenced_widgets":["3c7cc708d5fb4d81b0d93e22e817eb76","1a4414a957be4d7ea8b241ec4d86d4ad","346e19c6469e47bea6cfc4d6c1908613","3e470951305440469a78611f76936e89","a44e329567ef4883b6573c66795e7f76","409e355e2fca45f089abc9c311d5a662","4020227297f247d2aa88bc28e0854a9c","4f84a26cfeda46dc93a31bb24469cb2f","96cea2504a184898be9b2ac219c05c4c","d5ed76a1f3654972afcca92cdd8a7132","410150510bcc4e089f87c681fb8d922b"]},"executionInfo":{"elapsed":17517,"status":"ok","timestamp":1690784863305,"user":{"displayName":"abhishek kumar","userId":"11096944369498737237"},"user_tz":-330},"id":"BnQ2qVjnb-bo","outputId":"1fb2ab58-208e-4b4f-c1a9-08135d2494e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["================================================================================\n","Your GPU supports bfloat16: accelerate training with bf16=True\n","================================================================================\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3c7cc708d5fb4d81b0d93e22e817eb76","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Load dataset (you can process it here)\n","dataset = load_dataset(dataset_name, split=\"train\")\n","\n","# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","\n","# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    #report_to=\"tensorboard\"\n","    report_to=\"wandb\",  # enable logging to W&B\n","    run_name=\"llama-train-3\"  # name of the W&B run (optional)\n",")\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")"]},{"cell_type":"markdown","metadata":{"id":"qYCgI7Xhb-bo"},"source":["### Launch Training Job and Track the progress with W&B"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":298},"id":"dMi_9P3Mb-bo","outputId":"b03f7c57-7339-4a22-c719-4e734a2c849d"},"outputs":[{"data":{"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.15.7"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20230731_062746-s87g856b</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/explore-llm/fine-tuning/runs/s87g856b' target=\"_blank\">llama-train-3</a></strong> to <a href='https://wandb.ai/explore-llm/fine-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/explore-llm/fine-tuning' target=\"_blank\">https://wandb.ai/explore-llm/fine-tuning</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/explore-llm/fine-tuning/runs/s87g856b' target=\"_blank\">https://wandb.ai/explore-llm/fine-tuning/runs/s87g856b</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='74' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 74/250 01:20 < 03:15, 0.90 it/s, Epoch 0.29/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>1.249000</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.533700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# launch training job\n","trainer.train()\n","# finish the w&b tracking\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"RoZj5QRlb-bo"},"source":["### Persist Adapter Weights and combine with Base Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_QzcjOtCb-bo"},"outputs":[],"source":["# Save trained model\n","trainer.model.save_pretrained(new_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mWrAeQY-b-bo"},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F24nFNh3fCJu"},"outputs":[],"source":["!ls -l --block-size=M llama-2-7b-hf-v2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMMdNcHcb-bo"},"outputs":[],"source":["# Reload model in FP16 and merge it with LoRA weights\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map=device_map,\n",")\n","# create merged model\n","model = PeftModel.from_pretrained(base_model, new_model)\n","merged_model = model.merge_and_unload()\n","\n","# Reload tokenizer to save it\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjM2bZnvb-bo"},"outputs":[],"source":["# persist merged model weights\n","merged_model.save_pretrained(new_model,safe_serialization=True)\n","tokenizer.save_pretrained(new_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":587,"status":"ok","timestamp":1690783124173,"user":{"displayName":"abhishek kumar","userId":"11096944369498737237"},"user_tz":-330},"id":"Vbq_l0Rab-bo","outputId":"9faa636f-fbd6-4337-d892-03c6096e820f"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 12983M\n","-rw-r--r-- 1 root root    1M Jul 31 05:46 adapter_config.json\n","-rw-r--r-- 1 root root  129M Jul 31 05:46 adapter_model.bin\n","-rw-r--r-- 1 root root    1M Jul 31 05:48 config.json\n","-rw-r--r-- 1 root root    1M Jul 31 05:48 generation_config.json\n","-rw-r--r-- 1 root root 9515M Jul 31 05:49 model-00001-of-00002.safetensors\n","-rw-r--r-- 1 root root 3339M Jul 31 05:49 model-00002-of-00002.safetensors\n","-rw-r--r-- 1 root root    1M Jul 31 05:49 model.safetensors.index.json\n","-rw-r--r-- 1 root root    1M Jul 31 05:46 README.md\n","-rw-r--r-- 1 root root    1M Jul 31 05:49 special_tokens_map.json\n","-rw-r--r-- 1 root root    1M Jul 31 05:49 tokenizer_config.json\n","-rw-r--r-- 1 root root    2M Jul 31 05:49 tokenizer.json\n"]}],"source":["!ls -l --block-size=M llama-2-7b-hf-v2"]},{"cell_type":"markdown","metadata":{"id":"p6eeSbkqb-bp"},"source":["### Push Weights to GCS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21365,"status":"ok","timestamp":1690783514749,"user":{"displayName":"abhishek kumar","userId":"11096944369498737237"},"user_tz":-330},"id":"f0TkeUTbiTjA","outputId":"f9ad5de6-0a18-4eb2-c88f-26a65b276427"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":799,"status":"ok","timestamp":1690783673493,"user":{"displayName":"abhishek kumar","userId":"11096944369498737237"},"user_tz":-330},"id":"cNYwmVMcgexR","outputId":"a28f1ad9-9440-4074-9408-8c575889eed8"},"outputs":[],"source":["!gcloud auth activate-service-account --key-file=drive/MyDrive/gcp-service-account.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wyA4e7tRb-bp"},"outputs":[],"source":["!pip install -qqq gcsfs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HAYz0O1b-bp"},"outputs":[],"source":["import gcsfs\n","def upload_to_gcs(src_dir: str, gcs_dst: str):\n","    fs = gcsfs.GCSFileSystem(project='<YOUR_PROJECT_ID>', token='drive/MyDrive/gcp-service-account.json')\n","    fs.put(src_dir, gcs_dst, recursive=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":836234,"status":"ok","timestamp":1690784654129,"user":{"displayName":"abhishek kumar","userId":"11096944369498737237"},"user_tz":-330},"id":"6gnBLAK-b-bp","outputId":"407ce651-5105-405a-ea69-93c00c7d11ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["Uploading model weights to gs://ps-llm-collection/llm/llama-2-7b-hf-v2\n"]}],"source":["source = os.path.join(os.path.curdir, new_model)\n","destination = f\"<GCS_BUCKET>/{new_model}\"\n","print(f\"Uploading model weights to {destination}\")\n","upload_to_gcs(source, destination)"]},{"cell_type":"markdown","metadata":{"id":"wBXpO0L5b-bp"},"source":[]},{"cell_type":"markdown","metadata":{"id":"6deLBQcdb-bp"},"source":[]}],"metadata":{"colab":{"cell_execution_strategy":"setup","gpuType":"A100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"orig_nbformat":4,"widgets":{"application/vnd.jupyter.widget-state+json":{"1a4414a957be4d7ea8b241ec4d86d4ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_409e355e2fca45f089abc9c311d5a662","placeholder":"​","style":"IPY_MODEL_4020227297f247d2aa88bc28e0854a9c","value":"Loading checkpoint shards: 100%"}},"346e19c6469e47bea6cfc4d6c1908613":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f84a26cfeda46dc93a31bb24469cb2f","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_96cea2504a184898be9b2ac219c05c4c","value":2}},"3c7cc708d5fb4d81b0d93e22e817eb76":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1a4414a957be4d7ea8b241ec4d86d4ad","IPY_MODEL_346e19c6469e47bea6cfc4d6c1908613","IPY_MODEL_3e470951305440469a78611f76936e89"],"layout":"IPY_MODEL_a44e329567ef4883b6573c66795e7f76"}},"3e470951305440469a78611f76936e89":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5ed76a1f3654972afcca92cdd8a7132","placeholder":"​","style":"IPY_MODEL_410150510bcc4e089f87c681fb8d922b","value":" 2/2 [00:04&lt;00:00,  2.07s/it]"}},"4020227297f247d2aa88bc28e0854a9c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"409e355e2fca45f089abc9c311d5a662":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"410150510bcc4e089f87c681fb8d922b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f84a26cfeda46dc93a31bb24469cb2f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96cea2504a184898be9b2ac219c05c4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a44e329567ef4883b6573c66795e7f76":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5ed76a1f3654972afcca92cdd8a7132":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
